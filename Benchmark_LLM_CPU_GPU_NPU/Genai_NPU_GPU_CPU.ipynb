{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841c5de8-0560-44d9-a0a7-0426c8aab8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openvino-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6208920a-9353-4540-930a-cee40a730b1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install optimum-intel nncf==2.11 onnx==1.16.1\n",
    "!pip install --pre openvino==2024.3.0.dev20240807 openvino-tokenizers==2024.3.0.0.dev20240807 openvino-genai==2024.3.0.0.dev20240807 --extra-index-url https://storage.openvinotoolkit.org/simple/wheels/pre-release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5b5f55-9589-49c7-bd02-a386493972ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!optimum-cli export openvino -m microsoft/phi-2 --weight-format int4 --sym --group-size 128 --ratio 1.0 phi_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97ebfd7-fc81-4153-89ab-2c58f900f9cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!optimum-cli export openvino -m TinyLlama/TinyLlama-1.1B-Chat-v1.0 --weight-format int4 --sym --group-size 128 --ratio 1.0 TinyLlama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d912ff-8cd5-4f77-b83e-1b4aaa2429b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino as ov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83da9f3a-a88c-4002-8987-4ebf2b252fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ov.Core().available_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f94bb79-ace5-40f2-9f9d-f9d78b29f4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino_genai as ov_genai\n",
    "import time\n",
    "import psutil\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff2d00b-895c-404b-bdca-298180e76b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "process = psutil.Process()\n",
    "start_memory_mb = process.memory_info().rss / (1024 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f276c697-cfcd-4aa9-a3d8-e472b2929d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_memory_mb = process.memory_info().rss / (1024 * 1024)\n",
    "pipe_npu = ov_genai.LLMPipeline(\"TinyLlama\", \"NPU\")\n",
    "end_memory_mb = process.memory_info().rss / (1024 * 1024)\n",
    "memory_used_mb = end_memory_mb - start_memory_mb\n",
    "print(f\"Memory used during compilation on NPU: {memory_used_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e02c95b-1f1a-4173-bc37-244349c367b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_memory_mb = process.memory_info().rss / (1024 * 1024)\n",
    "pipe_gpu = ov_genai.LLMPipeline(\"TinyLlama\", \"GPU\")\n",
    "end_memory_mb = process.memory_info().rss / (1024 * 1024)\n",
    "memory_used_mb = end_memory_mb - start_memory_mb\n",
    "print(f\"Memory used during compilation on GPU: {memory_used_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c877eaab-5211-432f-a9b6-b443e8f5aa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_memory_mb = process.memory_info().rss / (1024 * 1024)\n",
    "pipe_cpu = ov_genai.LLMPipeline(\"TinyLlama\", \"CPU\")\n",
    "end_memory_mb = process.memory_info().rss / (1024 * 1024)\n",
    "memory_used_mb = end_memory_mb - start_memory_mb\n",
    "print(f\"Memory used during compilation on CPU: {memory_used_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd614db7-906d-4dad-9644-054be780f1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "LLMPipeline - takes care of compilation requirements specific to device, especially static size requirement for NPU\n",
    "start_chat - used here to transfer context from one device to other. Context includes all information including KV cache etc\n",
    "For generation on CPU, GPU and NPU individually, does not require start_chat. But used it to obtain metrics of prefill and decode time\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc83459-1c94-42c7-be68-1033865bf0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_with_context(input_text, gpu_pipe, npu_pipe, tokens_to_generate=50):\n",
    "    # Generate tokens using the GPU\n",
    "    gpu_generated_text = gpu_pipe.generate(input_text, max_new_tokens=1)\n",
    "\n",
    "    # Transfer context (hidden states, memory) from GPU to NPU\n",
    "    npu_pipe.start_chat(system_message=gpu_generated_text)\n",
    "\n",
    "    # Generate additional tokens using the NPU\n",
    "    npu_generated_text = npu_pipe.generate(input_text, max_new_tokens=tokens_to_generate)\n",
    "\n",
    "    return gpu_generated_text, npu_generated_text\n",
    "\n",
    "def perform_prefill_and_decode(input_text, gpu_pipe, npu_pipe, tokens_to_generate=25):\n",
    "    # Prefill stage (GPU)\n",
    "    start_time = time.time()\n",
    "    gpu_generated_text = gpu_pipe.generate(input_text, max_new_tokens=1)\n",
    "    end_time = time.time()\n",
    "    prefill_time = end_time - start_time\n",
    "\n",
    "    # Transfer context from GPU to NPU\n",
    "    start_time = time.time()\n",
    "    npu_pipe.start_chat(system_message=gpu_generated_text)\n",
    "    end_time = time.time()\n",
    "    transfer_time = end_time - start_time\n",
    "\n",
    "    # Decode stage (NPU)\n",
    "    start_time = time.time()\n",
    "    npu_generated_text = npu_pipe.generate(input_text, max_new_tokens=tokens_to_generate)\n",
    "    end_time = time.time()\n",
    "    decode_time = end_time - start_time\n",
    "        \n",
    "    return gpu_generated_text, npu_generated_text, prefill_time, transfer_time, decode_time\n",
    "\n",
    "def perform_cpu(input_text, cpu_pipe, tokens_to_generate=25):\n",
    "    \n",
    "    # Prefill stage\n",
    "    start_time = time.time()\n",
    "    cpu_generated_text = cpu_pipe.generate(input_text, max_new_tokens=1)\n",
    "    end_time = time.time()\n",
    "    prefill_time = end_time - start_time\n",
    "    \n",
    "    # Maintain context\n",
    "    start_time = time.time()\n",
    "    cpu_pipe.start_chat(system_message=cpu_generated_text)\n",
    "    end_time = time.time()\n",
    "    transfer_time = end_time - start_time\n",
    "\n",
    "    # Decode stage\n",
    "    start_time = time.time()\n",
    "    cpu_generated_text = cpu_pipe.generate(input_text, max_new_tokens=tokens_to_generate)\n",
    "    end_time = time.time()\n",
    "    decode_time = end_time - start_time\n",
    "        \n",
    "    return cpu_generated_text, prefill_time, transfer_time, decode_time\n",
    "\n",
    "def perform_gpu(input_text, gpu_pipe, tokens_to_generate=25):\n",
    "    \n",
    "    # Prefill stage\n",
    "    start_time = time.time()\n",
    "    gpu_generated_text = gpu_pipe.generate(input_text, max_new_tokens=1)\n",
    "    end_time = time.time()\n",
    "    prefill_time = end_time - start_time\n",
    "    \n",
    "    # Maintain context\n",
    "    start_time = time.time()\n",
    "    gpu_pipe.start_chat(system_message=gpu_generated_text)\n",
    "    end_time = time.time()\n",
    "    transfer_time = end_time - start_time\n",
    "\n",
    "    # Decode stage\n",
    "    start_time = time.time()\n",
    "    gpu_generated_text = gpu_pipe.generate(input_text, max_new_tokens=tokens_to_generate)\n",
    "    end_time = time.time()\n",
    "    decode_time = end_time - start_time\n",
    "        \n",
    "    return gpu_generated_text, prefill_time, transfer_time, decode_time\n",
    "\n",
    "def perform_npu(input_text, npu_pipe, tokens_to_generate=25):\n",
    "    \n",
    "    # Prefill stage\n",
    "    start_time = time.time()\n",
    "    npu_generated_text = npu_pipe.generate(input_text, max_new_tokens=1)\n",
    "    end_time = time.time()\n",
    "    prefill_time = end_time - start_time\n",
    "    \n",
    "    # Maintain context\n",
    "    start_time = time.time()\n",
    "    npu_pipe.start_chat(system_message=npu_generated_text)\n",
    "    end_time = time.time()\n",
    "    transfer_time = end_time - start_time\n",
    "\n",
    "    # Decode stage\n",
    "    start_time = time.time()\n",
    "    npu_generated_text = npu_pipe.generate(input_text, max_new_tokens=tokens_to_generate)\n",
    "    end_time = time.time()\n",
    "    decode_time = end_time - start_time\n",
    "        \n",
    "    return npu_generated_text, prefill_time, transfer_time, decode_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd1f0db-258d-4594-8104-6f46f1faa4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get metrics on NPU\n",
    "input_text = \"The Sun is yellow because\"\n",
    "start_memory_mb = process.memory_info().rss / (1024 * 1024)\n",
    "npu_generated_text, prefill_time, transfer_time, decode_time = perform_npu(input_text, pipe_npu)\n",
    "end_memory_mb = process.memory_info().rss / (1024 * 1024)\n",
    "memory_used_mb = end_memory_mb - start_memory_mb\n",
    "pf_time = []\n",
    "dc_time = []\n",
    "tf_time = []\n",
    "out_tokens = []\n",
    "pf_time.append(prefill_time)\n",
    "dc_time.append(decode_time)\n",
    "tf_time.append(transfer_time)\n",
    "print(\"NPU Generated Text: \")\n",
    "print(npu_generated_text)\n",
    "out_tokens.append(len(npu_generated_text.split()))\n",
    "for _ in range(10):\n",
    "    npu_generated_text, prefill_time, transfer_time, decode_time = perform_npu(input_text, pipe_npu)\n",
    "    pf_time.append(prefill_time)\n",
    "    dc_time.append(decode_time)\n",
    "    tf_time.append(transfer_time)\n",
    "    out_tokens.append(len(npu_generated_text.split()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4689d90-628c-432b-8c16-a3af44f4e0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Current metrics of total input and output tokens is approximate as split() is being used\n",
    "Should change it to get accurate values\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e88d8e-9bbd-40a8-92bc-2db486e79b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Memory used {memory_used_mb:.2f} MB\") \n",
    "print(\"Prefill\")\n",
    "print(statistics.mean(pf_time))\n",
    "print(\"Transfer\")\n",
    "print(statistics.mean(tf_time))\n",
    "print(\"Decode\")\n",
    "print(statistics.mean(dc_time))\n",
    "print(\"Generated tokens\")\n",
    "print(statistics.mean(out_tokens))\n",
    "avg_tokens_per_second = statistics.mean(out_tokens) / (statistics.mean(pf_time) + statistics.mean(dc_time) + statistics.mean(tf_time))\n",
    "print(\"Average tokens/second\")\n",
    "print(avg_tokens_per_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a288eca-688d-4825-ad62-05bd38e81d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get metrics on GPU\n",
    "input_text = \"The Sun is yellow because\"\n",
    "start_memory_mb = process.memory_info().rss / (1024 * 1024)\n",
    "gpu_generated_text, prefill_time, transfer_time, decode_time = perform_gpu(input_text, pipe_gpu)\n",
    "end_memory_mb = process.memory_info().rss / (1024 * 1024)\n",
    "memory_used_mb = end_memory_mb - start_memory_mb\n",
    "print(\"GPU Generated Text: \")\n",
    "print(gpu_generated_text)\n",
    "pfg_time = []\n",
    "dcg_time = []\n",
    "tfg_time = []\n",
    "out_tokens = []\n",
    "pfg_time.append(prefill_time)\n",
    "dcg_time.append(decode_time)\n",
    "tfg_time.append(transfer_time)\n",
    "out_tokens.append(len(gpu_generated_text.split()))\n",
    "for _ in range(10):\n",
    "    gpu_generated_text, prefill_time, transfer_time, decode_time = perform_gpu(input_text, pipe_gpu)\n",
    "    pfg_time.append(prefill_time)\n",
    "    dcg_time.append(decode_time)\n",
    "    tfg_time.append(transfer_time)\n",
    "    out_tokens.append(len(gpu_generated_text.split()))\n",
    "print(f\"Memory used {memory_used_mb:.2f} MB\") \n",
    "print(\"Prefill\")\n",
    "print(statistics.mean(pfg_time))\n",
    "print(\"Transfer\")\n",
    "print(statistics.mean(tfg_time))\n",
    "print(\"Decode\")\n",
    "print(statistics.mean(dcg_time))\n",
    "print(\"Generated tokens\")\n",
    "print(statistics.mean(out_tokens))\n",
    "avg_tokens_per_second = statistics.mean(out_tokens) / (statistics.mean(pfg_time) + statistics.mean(dcg_time) + statistics.mean(tfg_time))\n",
    "print(\"Average tokens/second\")\n",
    "print(avg_tokens_per_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bbb1f8-bfde-4a34-95cc-17c214abdfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get metrics on CPU\n",
    "input_text = \"The Sun is yellow because\"\n",
    "start_memory_mb = process.memory_info().rss / (1024 * 1024)\n",
    "cpu_generated_text, prefill_time, transfer_time, decode_time = perform_cpu(input_text, pipe_cpu)\n",
    "end_memory_mb = process.memory_info().rss / (1024 * 1024)\n",
    "memory_used_mb = end_memory_mb - start_memory_mb\n",
    "print(\"CPU Generated Text: \")\n",
    "print(cpu_generated_text)\n",
    "pfc_time = []\n",
    "dcc_time = []\n",
    "tfc_time = []\n",
    "out_tokens = []\n",
    "pfc_time.append(prefill_time)\n",
    "dcc_time.append(decode_time)\n",
    "tfc_time.append(transfer_time)\n",
    "out_tokens.append(len(cpu_generated_text.split()))\n",
    "for _ in range(10):\n",
    "    cpu_generated_text, prefill_time, transfer_time, decode_time = perform_cpu(input_text, pipe_cpu)\n",
    "    pfc_time.append(prefill_time)\n",
    "    dcc_time.append(decode_time)\n",
    "    tfc_time.append(transfer_time)\n",
    "    out_tokens.append(len(cpu_generated_text.split()))\n",
    "print(f\"Memory used {memory_used_mb:.2f} MB\") \n",
    "print(\"Prefill\")\n",
    "print(statistics.mean(pfc_time))\n",
    "print(\"Transfer\")\n",
    "print(statistics.mean(tfc_time))\n",
    "print(\"Decode\")\n",
    "print(statistics.mean(dcc_time))\n",
    "print(\"Generated tokens\")\n",
    "print(statistics.mean(out_tokens))\n",
    "avg_tokens_per_second = statistics.mean(out_tokens) / (statistics.mean(pfc_time) + statistics.mean(dcc_time) + statistics.mean(tfc_time))\n",
    "print(\"Average tokens/second\")\n",
    "print(avg_tokens_per_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0b4c0f-e0b4-4b4a-aec8-5837c8d0a544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get metrics on CPU+NPU\n",
    "pipe_npu = ov_genai.LLMPipeline(\"TinyLlama\", \"NPU\")\n",
    "pipe_cpu = ov_genai.LLMPipeline(\"TinyLlama\", \"CPU\")\n",
    "input_text = \"The Sun is yellow because\"\n",
    "start_memory_mb = process.memory_info().rss / (1024 * 1024)\n",
    "npu_generated_text, cpu_generated_text, prefill_time, transfer_time, decode_time = perform_prefill_and_decode(input_text, pipe_npu, pipe_cpu)\n",
    "end_memory_mb = process.memory_info().rss / (1024 * 1024)\n",
    "memory_used_mb = end_memory_mb - start_memory_mb\n",
    "print(\"CPU + NPU Generated Text(NPU - Prefill, CPU - Decode): \")\n",
    "print(cpu_generated_text)\n",
    "pfcn_time = []\n",
    "dccn_time = []\n",
    "tfcn_time = []\n",
    "out_tokens = []\n",
    "pfcn_time.append(prefill_time)\n",
    "dccn_time.append(decode_time)\n",
    "tfcn_time.append(transfer_time)\n",
    "out_tokens.append(len(cpu_generated_text.split()))\n",
    "for _ in range(10):\n",
    "    npu_generated_text, cpu_generated_text, prefill_time, transfer_time, decode_time = perform_prefill_and_decode(input_text, pipe_npu, pipe_cpu)\n",
    "    pfcn_time.append(prefill_time)\n",
    "    dccn_time.append(decode_time)\n",
    "    tfcn_time.append(transfer_time)\n",
    "    out_tokens.append(len(cpu_generated_text.split()))\n",
    "print(f\"Memory used {memory_used_mb:.2f} MB\") \n",
    "print(\"Prefill\")\n",
    "print(statistics.mean(pfcn_time))\n",
    "print(\"Transfer\")\n",
    "print(statistics.mean(tfcn_time))\n",
    "print(\"Decode\")\n",
    "print(statistics.mean(dccn_time))\n",
    "print(\"Generated tokens\")\n",
    "print(statistics.mean(out_tokens))\n",
    "avg_tokens_per_second = statistics.mean(out_tokens) / (statistics.mean(pfcn_time) + statistics.mean(dccn_time) + statistics.mean(tfcn_time))\n",
    "print(\"Average tokens/second\")\n",
    "print(avg_tokens_per_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07d3217-42eb-4b27-98ac-e7594b824428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get metrics on CPU+GPU\n",
    "pipe_gpu = ov_genai.LLMPipeline(\"TinyLlama\", \"GPU\")\n",
    "pipe_cpu = ov_genai.LLMPipeline(\"TinyLlama\", \"CPU\")\n",
    "input_text = \"The Sun is yellow because\"\n",
    "start_memory_mb = process.memory_info().rss / (1024 * 1024)\n",
    "gpu_generated_text, cpu_generated_text, prefill_time, transfer_time, decode_time = perform_prefill_and_decode(input_text, pipe_gpu, pipe_cpu)\n",
    "end_memory_mb = process.memory_info().rss / (1024 * 1024)\n",
    "memory_used_mb = end_memory_mb - start_memory_mb\n",
    "print(\"CPU + GPU Generated Text(GPU - Prefill, CPU - Decode): \")\n",
    "print(cpu_generated_text)\n",
    "pfcg_time = []\n",
    "dccg_time = []\n",
    "tfcg_time = []\n",
    "out_tokens = []\n",
    "pfcg_time.append(prefill_time)\n",
    "dccg_time.append(decode_time)\n",
    "tfcg_time.append(transfer_time)\n",
    "out_tokens.append(len(cpu_generated_text.split()))\n",
    "for _ in range(10):\n",
    "    gpu_generated_text, cpu_generated_text, prefill_time, transfer_time, decode_time = perform_prefill_and_decode(input_text, pipe_gpu, pipe_cpu)\n",
    "    pfcg_time.append(prefill_time)\n",
    "    dccg_time.append(decode_time)\n",
    "    tfcg_time.append(transfer_time)\n",
    "    out_tokens.append(len(cpu_generated_text.split()))\n",
    "print(f\"Memory used {memory_used_mb:.2f} MB\") \n",
    "print(\"Prefill\")\n",
    "print(statistics.mean(pfcg_time))\n",
    "print(\"Transfer\")\n",
    "print(statistics.mean(tfcg_time))\n",
    "print(\"Decode\")\n",
    "print(statistics.mean(dccg_time))\n",
    "print(\"Generated tokens\")\n",
    "print(statistics.mean(out_tokens))\n",
    "avg_tokens_per_second = statistics.mean(out_tokens) / (statistics.mean(pfcg_time) + statistics.mean(dccg_time) + statistics.mean(tfcg_time))\n",
    "print(\"Average tokens/second\")\n",
    "print(avg_tokens_per_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fa0eab-a7d9-4d03-a84e-1060a19bc2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get metrics on GPU+NPU\n",
    "pipe_npu = ov_genai.LLMPipeline(\"TinyLlama\", \"NPU\")\n",
    "pipe_gpu = ov_genai.LLMPipeline(\"TinyLlama\", \"GPU\")\n",
    "input_text = \"The Sun is yellow because\"\n",
    "start_memory_mb = process.memory_info().rss / (1024 * 1024)\n",
    "gpu_generated_text, npu_generated_text, prefill_time, transfer_time, decode_time = perform_prefill_and_decode(input_text, pipe_gpu, pipe_npu)\n",
    "end_memory_mb = process.memory_info().rss / (1024 * 1024)\n",
    "memory_used_mb = end_memory_mb - start_memory_mb\n",
    "print(\"GPU + NPU Generated Text(GPU - Prefill, NPU - Decode): \")\n",
    "print(npu_generated_text)\n",
    "pfng_time = []\n",
    "dcng_time = []\n",
    "tfng_time = []\n",
    "out_tokens = []\n",
    "pfng_time.append(prefill_time)\n",
    "dcng_time.append(decode_time)\n",
    "tfng_time.append(transfer_time)\n",
    "out_tokens.append(len(npu_generated_text.split()))\n",
    "for _ in range(10):\n",
    "    gpu_generated_text, npu_generated_text, prefill_time, transfer_time, decode_time = perform_prefill_and_decode(input_text, pipe_gpu, pipe_npu)\n",
    "    pfng_time.append(prefill_time)\n",
    "    dcng_time.append(decode_time)\n",
    "    tfng_time.append(transfer_time)\n",
    "    out_tokens.append(len(npu_generated_text.split()))\n",
    "print(f\"Memory used {memory_used_mb:.2f} MB\") \n",
    "print(\"Prefill\")\n",
    "print(statistics.mean(pfng_time))\n",
    "print(\"Transfer\")\n",
    "print(statistics.mean(tfng_time))\n",
    "print(\"Decode\")\n",
    "print(statistics.mean(dcng_time))\n",
    "print(\"Generated tokens\")\n",
    "print(statistics.mean(out_tokens))\n",
    "avg_tokens_per_second = statistics.mean(out_tokens) / (statistics.mean(pfng_time) + statistics.mean(dcng_time) + statistics.mean(tfng_time))\n",
    "print(\"Average tokens/second\")\n",
    "print(avg_tokens_per_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e63a33c-e2e2-4294-bfa9-bb81dba777ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f3132c-a09f-49a3-90e2-381982846370",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd5e1e9-8cfe-4414-93d6-45a143ae6cca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5adc22-f77f-4408-a6f0-badd3bf04af7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5e8891-54e8-48c0-b5b8-dc097932cfda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dadca20-a13c-430f-a658-156bf49e19c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b291a08-4048-4a5e-9f61-cc20ea49aeb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a8243f-5018-4c64-bc58-3088a43dabd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09d9fec-dd40-401a-8a75-116689fafdd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a169adeb-c7d6-481b-87ab-857c63ca03eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
